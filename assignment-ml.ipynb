{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8924143,"sourceType":"datasetVersion","datasetId":5367859},{"sourceId":2624,"sourceType":"modelInstanceVersion","modelInstanceId":1900},{"sourceId":2938,"sourceType":"modelInstanceVersion","modelInstanceId":2180}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"lets code new","metadata":{}},{"cell_type":"markdown","source":"### Sentiment Analysis Model Documentation\n\n#### Objective:\nThe objective of this assignment is to develop a sentiment analysis model that classifies text into multiple emotion categories using the GoEmotions dataset. The model should accurately predict the emotion conveyed in a given piece of text.\n\n#### Requirements:\n1. **Model Development**: Build a machine learning model capable of multi-label classification to predict emotions in text.\n2. **Performance Metrics**: Evaluate the model using appropriate performance metrics such as accuracy, precision, recall, F1-score, and AUC-ROC.\n3. **Documentation**: Provide a clear and concise report detailing the methodology, model architecture, hyperparameter tuning, and evaluation results.\n4. **Code Submission**: Submit well-documented code with clear instructions on how to run the model and reproduce the results.\n\n### Methodology\n\n#### 1. Data Preparation:\n- **Dataset**: The GoEmotions dataset was used for training and evaluation. The dataset contains multiple emotion labels for each piece of text.\n- **Data Preprocessing**: The text data was preprocessed to remove any unwanted characters and tokens. The `Processed_Text` column in the dataset was used for training the models.\n\n#### 2. Feature Extraction:\n- **TF-IDF Vectorization**: The text data was converted into numerical features using the TF-IDF (Term Frequency-Inverse Document Frequency) vectorizer. The vectorizer was configured to use a maximum of 5000 features and n-grams ranging from 1 to 2.\n\n#### 3. Model Development:\nThree different models were developed and trained:\n\n- **LSTM Model**:\n  - **Architecture**: An LSTM (Long Short-Term Memory) network with an embedding layer, an LSTM layer, a global max-pooling layer, and dense layers.\n  - **Training**: The model was trained using binary cross-entropy loss and the Adam optimizer.\n  - **Evaluation**: The model was evaluated using accuracy, precision, recall, F1-score, Hamming loss, and AUC-ROC.\n\n- **Traditional Models**:\n  - **SVM (Support Vector Machine)**: An SVM model with a linear kernel and One-vs-Rest classification.\n  - **Logistic Regression**: A logistic regression model with One-vs-Rest classification.\n  - **Random Forest**: A random forest classifier with One-vs-Rest classification.\n  - **Training**: Each model was trained on the TF-IDF features.\n  - **Evaluation**: Each model was evaluated using accuracy, precision, recall, F1-score, Hamming loss, and AUC-ROC.\n\n#### 4. Hyperparameter Tuning:\nHyperparameters for each model were tuned to achieve the best performance. The details of the hyperparameters and the tuning process are included in the code.\n\n### Model Evaluation\n\nThe models were evaluated on the validation dataset using the following metrics:\n- **Accuracy**: Measures the proportion of correctly classified instances.\n- **Precision**: Measures the proportion of true positive instances among the instances classified as positive.\n- **Recall**: Measures the proportion of true positive instances among all actual positive instances.\n- **F1-Score**: The harmonic mean of precision and recall.\n- **Hamming Loss**: The fraction of labels that are incorrectly predicted.\n- **AUC-ROC**: Measures the area under the ROC curve, providing an aggregate measure of performance across all classification thresholds.\n\n### Code Instructions\n\n#### 1. Dependencies:\nEnsure you have the following libraries installed:\n- TensorFlow\n- Scikit-learn\n- NumPy\n- Pandas\n\nYou can install the required libraries using the following command:\n```sh\npip install tensorflow scikit-learn numpy pandas vaderSentiment tensorflow_text wurlitzer optuna\n```\n\n","metadata":{}},{"cell_type":"markdown","source":"## Setup and Package Installation\nThis block installs the `vaderSentiment` package and handles potential issues related to multithreading.\n","metadata":{}},{"cell_type":"code","source":"pip install vaderSentiment","metadata":{"execution":{"iopub.status.busy":"2024-07-14T13:16:00.891926Z","iopub.execute_input":"2024-07-14T13:16:00.892293Z","iopub.status.idle":"2024-07-14T13:16:13.380731Z","shell.execute_reply.started":"2024-07-14T13:16:00.892265Z","shell.execute_reply":"2024-07-14T13:16:13.379596Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: vaderSentiment in /opt/conda/lib/python3.10/site-packages (3.3.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from vaderSentiment) (2.32.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->vaderSentiment) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->vaderSentiment) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->vaderSentiment) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->vaderSentiment) (2024.7.4)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Loading and Displaying Data\nThis block loads the training and development datasets from TSV files without headers and assigns column names. It also reads auxiliary files like `ekman_labels`, `emotions`, `sentiment_dict`, `sentiment_mapping`, and `ekman_mapping` for additional processing. Finally, it displays the first few rows of the loaded data to verify correctness.\n","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, hamming_loss\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, GlobalMaxPool1D\nimport tensorflow as tf\nimport json\n\nnltk.download('stopwords')\nnltk.download('punkt')\n\n# Load data without headers and assign column names\ntrain_data = pd.read_csv('/kaggle/input/assignment/data/train.tsv', sep='\\t', header=None, names=['Text', 'Num', 'ID'])\ndev_data = pd.read_csv('/kaggle/input/assignment/data/dev.tsv', sep='\\t', header=None, names=['Text', 'Num', 'ID'])\n\n# Display first few rows to verify\nprint(\"Train Data Columns:\", train_data.columns)\nprint(train_data.head())\n\nprint(\"Dev Data Columns:\", dev_data.columns)\nprint(dev_data.head())\n\nekman_labels = pd.read_csv('/kaggle/input/assignment/data/ekman_labels.csv')\nwith open('/kaggle/input/assignment/data/emotions.txt') as f:\n    emotions = f.read().splitlines()\nwith open('/kaggle/input/assignment/data/sentiment_dict.json') as f:\n    sentiment_dict = json.load(f)\nwith open('/kaggle/input/assignment/data/sentiment_mapping.json') as f:\n    sentiment_mapping = json.load(f)\nwith open('/kaggle/input/assignment/data/ekman_mapping.json') as f:\n    ekman_mapping = json.load(f)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T13:16:13.382848Z","iopub.execute_input":"2024-07-14T13:16:13.383177Z","iopub.status.idle":"2024-07-14T13:16:13.638740Z","shell.execute_reply.started":"2024-07-14T13:16:13.383148Z","shell.execute_reply":"2024-07-14T13:16:13.637660Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nTrain Data Columns: Index(['Text', 'Num', 'ID'], dtype='object')\n                                                Text Num       ID\n0  My favourite food is anything I didn't have to...  27  eebbqej\n1  Now if he does off himself, everyone will thin...  27  ed00q6i\n2                     WHY THE FUCK IS BAYLESS ISOING   2  eezlygj\n3                        To make her feel threatened  14  ed7ypvh\n4                             Dirty Southern Wankers   3  ed0bdzj\nDev Data Columns: Index(['Text', 'Num', 'ID'], dtype='object')\n                                                Text   Num       ID\n0  Is this in New Orleans?? I really feel like th...    27  edgurhb\n1  You know the answer man, you are programmed to...  4,27  ee84bjg\n2               I've never been this sad in my life!    25  edcu99z\n3  The economy is heavily controlled and subsidiz...  4,27  edc32e2\n4  He could have easily taken a real camera from ...    20  eepig6r\n","output_type":"stream"}]},{"cell_type":"markdown","source":"\n## Data Preprocessing for Multi-Label Text Classification\n\n### Overview\nThis block preprocesses the text data and transforms the labels for multi-label classification. The steps include text cleaning, tokenization, stopword removal, and label binarization using `MultiLabelBinarizer`.\n\n### Text Preprocessing\nThe `preprocess_text` function performs several key text preprocessing steps:\n1. **Lowercasing**: Converts all characters in the text to lowercase to ensure uniformity.\n2. **Removing Non-Alphabetic Characters**: Uses regular expressions to remove any characters that are not letters or whitespace.\n3. **Tokenization**: Splits the text into individual words (tokens).\n4. **Stopword Removal**: Removes common English stopwords using NLTK's stopword list to reduce noise and focus on meaningful words.\n\n```python\n# Data Preprocessing\ndef preprocess_text(text):\n    text = text.lower()\n    text = re.sub(r'[^a-z\\s]', '', text)\n    tokens = nltk.word_tokenize(text)\n    tokens = [token for token in tokens if token not in stopwords.words('english')]\n    return ' '.join(tokens)\n\ntrain_data['Processed_Text'] = train_data['Text'].apply(preprocess_text)\ndev_data['Processed_Text'] = dev_data['Text'].apply(preprocess_text)\n```\n\n### Label Transformation\nMulti-label classification requires labels to be in a binary format where each possible label is represented as a binary vector. We use `MultiLabelBinarizer` to transform the `Num` column into this format.\n\n1. **Label Conversion**: Converts the `Num` column from a string of comma-separated numbers to a list of integers.\n2. **Binarization**: Transforms the lists of integers into a binary format where each position in the vector represents the presence (1) or absence (0) of a label.\n\n```python\n# Multi-Label Binarizer for the labels\nmlb = MultiLabelBinarizer(classes=range(28))  # Assuming there are 28 emotions\ntrain_data['Num'] = train_data['Num'].apply(lambda x: list(map(int, x.split(','))))\ndev_data['Num'] = dev_data['Num'].apply(lambda x: list(map(int, x.split(','))))\n\ny_train = mlb.fit_transform(train_data['Num'])\ny_dev = mlb.transform(dev_data['Num'])\n```\n\n### Explanation and Benefits\n- **Lowercasing**: Ensures that the model treats words like \"The\" and \"the\" as the same word, reducing the dimensionality of the text data.\n- **Removing Non-Alphabetic Characters**: Eliminates punctuation and special characters, which are generally not useful for text classification and can add noise.\n- **Tokenization**: Splits text into individual words, which are the basic units of analysis in most NLP tasks.\n- **Stopword Removal**: Removes common words that do not contribute significantly to the meaning of the text, helping to focus on more informative words.\n- **Label Binarization**: Converts labels into a format suitable for multi-label classification models, where each label is represented as a binary vector.\n\nThis preprocessing step is crucial for preparing the text and labels in a format that can be effectively used by machine learning models for multi-label classification.\n\n","metadata":{}},{"cell_type":"code","source":"# Data Preprocessing\ndef preprocess_text(text):\n    text = text.lower()\n    text = re.sub(r'[^a-z\\s]', '', text)\n    tokens = nltk.word_tokenize(text)\n    tokens = [token for token in tokens if token not in stopwords.words('english')]\n    return ' '.join(tokens)\n\ntrain_data['Processed_Text'] = train_data['Text'].apply(preprocess_text)\ndev_data['Processed_Text'] = dev_data['Text'].apply(preprocess_text)\n\n# Multi-Label Binarizer for the labels\nmlb = MultiLabelBinarizer(classes=range(28))  # Assuming there are 28 emotions\ntrain_data['Num'] = train_data['Num'].apply(lambda x: list(map(int, x.split(','))))\ndev_data['Num'] = dev_data['Num'].apply(lambda x: list(map(int, x.split(','))))\n\ny_train = mlb.fit_transform(train_data['Num'])\ny_dev = mlb.transform(dev_data['Num'])","metadata":{"execution":{"iopub.status.busy":"2024-07-14T13:16:13.639950Z","iopub.execute_input":"2024-07-14T13:16:13.640236Z","iopub.status.idle":"2024-07-14T13:17:39.802031Z","shell.execute_reply.started":"2024-07-14T13:16:13.640211Z","shell.execute_reply":"2024-07-14T13:17:39.801242Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"\n## Traditional Machine Learning Models for Multi-Label Text Classification\n\n### Overview\nThis block implements traditional machine learning models for multi-label text classification using the TF-IDF vectorization technique. The workflow includes TF-IDF vectorization of the text data, defining three different models (SVM, Logistic Regression, and Random Forest) with `OneVsRestClassifier` for multi-label classification, training these models, and evaluating their performance on the development data.\n\n### TF-IDF Vectorization\nTF-IDF (Term Frequency-Inverse Document Frequency) is a numerical statistic that reflects the importance of a word in a document relative to a corpus. It is widely used in text mining and information retrieval to convert text data into numerical vectors.\n\n```python\n# TF-IDF Vectorization\nvectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\nX_train = vectorizer.fit_transform(train_data['Processed_Text'])\nX_dev = vectorizer.transform(dev_data['Processed_Text'])\n```\n\n### Model Definitions\nWe define three traditional machine learning models using `OneVsRestClassifier` for multi-label classification:\n1. **SVM (Support Vector Machine)**: Effective in high-dimensional spaces and for cases where the number of dimensions exceeds the number of samples.\n2. **Logistic Regression**: Suitable for binary classification tasks and extended to multi-label tasks using `OneVsRestClassifier`.\n3. **Random Forest**: An ensemble learning method that operates by constructing multiple decision trees during training.\n\n#### SVM Model\n```python\n# Define SVM model with OneVsRestClassifier\nsvm_model = OneVsRestClassifier(SVC(kernel='linear', probability=True, random_state=42))\n```\n\n#### Logistic Regression Model\n```python\n# Define Logistic Regression model with OneVsRestClassifier\nlogistic_model = OneVsRestClassifier(LogisticRegression(random_state=42))\n```\n\n#### Random Forest Model\n```python\n# Define Random Forest model with OneVsRestClassifier\nrf_model = OneVsRestClassifier(RandomForestClassifier(random_state=42))\n```\n\n### Model Training\nThe models are trained on the TF-IDF vectorized training data.\n\n```python\n# Train models\nprint(\"training svm model\")\nsvm_model.fit(X_train, y_train)\nprint(\"training Logistic model\")\nlogistic_model.fit(X_train, y_train)\nprint(\"training rf model\")\nrf_model.fit(X_train, y_train)\n```\n\n### Model Evaluation\nThe trained models are evaluated on the development data using the following metrics:\n- **Accuracy**: The proportion of correctly predicted labels.\n- **Precision (Weighted)**: The proportion of true positive predictions among all positive predictions, weighted by the number of true instances for each label.\n- **Recall (Weighted)**: The proportion of true positive predictions among all actual positives, weighted by the number of true instances for each label.\n- **F1 Score (Weighted)**: The harmonic mean of precision and recall, weighted by the number of true instances for each label.\n- **Hamming Loss**: The fraction of incorrect labels to the total number of labels.\n- **AUC-ROC (Weighted)**: The area under the receiver operating characteristic curve, averaged across labels and weighted by the number of true instances for each label.\n\n```python\n# Evaluate models\ndef evaluate_model(model, X_dev, y_dev, model_name):\n    predictions = model.predict(X_dev)\n    proba = model.predict_proba(X_dev)\n    accuracy = accuracy_score(y_dev, predictions)\n    precision = precision_score(y_dev, predictions, average='weighted', zero_division=0)\n    recall = recall_score(y_dev, predictions, average='weighted', zero_division=0)\n    f1 = f1_score(y_dev, predictions, average='weighted', zero_division=0)\n    hamming = hamming_loss(y_dev, predictions)\n    \n    # Calculate AUC-ROC for each label and average it\n    auc_roc = roc_auc_score(y_dev, proba, average='weighted', multi_class='ovr')\n    \n    print(f\"{model_name} Model Accuracy:\", accuracy)\n    print(f\"{model_name} Model Precision:\", precision)\n    print(f\"{model_name} Model Recall:\", recall)\n    print(f\"{model_name} Model F1 Score:\", f1)\n    print(f\"{model_name} Model Hamming Loss:\", hamming)\n    print(f\"{model_name} Model AUC-ROC:\", auc_roc)\n\nevaluate_model(svm_model, X_dev, y_dev, 'SVM')\nevaluate_model(logistic_model, X_dev, y_dev, 'Logistic Regression')\nevaluate_model(rf_model, X_dev, y_dev, 'Random Forest')\n```\n\n### Explanation and Benefits\n- **SVM (Support Vector Machine)**:\n  - **Benefits**: Effective in high-dimensional spaces, robust to overfitting in high-dimensional space, especially when the number of dimensions is greater than the number of samples.\n  - **Use Case**: Suitable for text classification tasks where the data is high-dimensional.\n- **Logistic Regression**:\n  - **Benefits**: Simple, easy to implement, and interpretable. It works well when the relationship between features and labels is approximately linear.\n  - **Use Case**: Effective for binary classification and extended to multi-label classification using `OneVsRestClassifier`.\n- **Random Forest**:\n  - **Benefits**: Handles large datasets with higher dimensionality. It provides better accuracy and handles overfitting better than individual decision trees.\n  - **Use Case**: Suitable for both classification and regression tasks and robust against overfitting.\n\nUsing traditional machine learning models provides a baseline for multi-label text classification, allowing comparison with more complex models like LSTM or BERT.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, hamming_loss, roc_auc_score\n\n# TF-IDF Vectorization\nvectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\nX_train = vectorizer.fit_transform(train_data['Processed_Text'])\nX_dev = vectorizer.transform(dev_data['Processed_Text'])\n\n# Define traditional models with OneVsRestClassifier\nsvm_model = OneVsRestClassifier(SVC(kernel='linear', probability=True, random_state=42))\nlogistic_model = OneVsRestClassifier(LogisticRegression(random_state=42))\nrf_model = OneVsRestClassifier(RandomForestClassifier(random_state=42))\n\n# Train models\nprint(\"training svm model\")\nsvm_model.fit(X_train, y_train)\nprint(\"training Logistic model\")\nlogistic_model.fit(X_train, y_train)\nprint(\"training rf model\")\nrf_model.fit(X_train, y_train)\n\n# Evaluate models\ndef evaluate_model(model, X_dev, y_dev, model_name):\n    predictions = model.predict(X_dev)\n    proba = model.predict_proba(X_dev)\n    accuracy = accuracy_score(y_dev, predictions)\n    precision = precision_score(y_dev, predictions, average='weighted', zero_division=0)\n    recall = recall_score(y_dev, predictions, average='weighted', zero_division=0)\n    f1 = f1_score(y_dev, predictions, average='weighted', zero_division=0)\n    hamming = hamming_loss(y_dev, predictions)\n    \n    # Calculate AUC-ROC for each label and average it\n    auc_roc = roc_auc_score(y_dev, proba, average='weighted', multi_class='ovr')\n    \n    print(f\"{model_name} Model Accuracy:\", accuracy)\n    print(f\"{model_name} Model Precision:\", precision)\n    print(f\"{model_name} Model Recall:\", recall)\n    print(f\"{model_name} Model F1 Score:\", f1)\n    print(f\"{model_name} Model Hamming Loss:\", hamming)\n    print(f\"{model_name} Model AUC-ROC:\", auc_roc)\n\nevaluate_model(svm_model, X_dev, y_dev, 'SVM')\nevaluate_model(logistic_model, X_dev, y_dev, 'Logistic Regression')\nevaluate_model(rf_model, X_dev, y_dev, 'Random Forest')\n","metadata":{"execution":{"iopub.status.busy":"2024-07-14T11:32:09.875723Z","iopub.execute_input":"2024-07-14T11:32:09.876014Z","iopub.status.idle":"2024-07-14T13:05:58.370185Z","shell.execute_reply.started":"2024-07-14T11:32:09.875990Z","shell.execute_reply":"2024-07-14T13:05:58.369156Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"training svm model\ntraining Logistic model\ntraining rf model\nSVM Model Accuracy: 0.34998157021747145\nSVM Model Precision: 0.7203559062584777\nSVM Model Recall: 0.3719435736677116\nSVM Model F1 Score: 0.4306059018805341\nSVM Model Hamming Loss: 0.033028803117266074\nSVM Model AUC-ROC: 0.8100243155591815\nLogistic Regression Model Accuracy: 0.29819388131220054\nLogistic Regression Model Precision: 0.6651358620894415\nLogistic Regression Model Recall: 0.30783699059561126\nLogistic Regression Model F1 Score: 0.3775667878970923\nLogistic Regression Model Hamming Loss: 0.03447685745879627\nLogistic Regression Model AUC-ROC: 0.8477744911346823\nRandom Forest Model Accuracy: 0.33542204201990417\nRandom Forest Model Precision: 0.602984794147717\nRandom Forest Model Recall: 0.3721003134796238\nRandom Forest Model F1 Score: 0.4285463168437339\nRandom Forest Model Hamming Loss: 0.034285977568321836\nRandom Forest Model AUC-ROC: 0.8053787113199676\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## LSTM Model for Multi-Label Text Classification\n\n### Overview\nThis block implements an LSTM (Long Short-Term Memory) model using TensorFlow Keras for multi-label text classification. The process includes tokenizing and padding the sequences, defining the LSTM model, training it on the training data, and evaluating its performance on the development data.\n\n### Tokenization and Padding\nWe use the `Tokenizer` from Keras to convert the text into sequences of integers, where each integer represents a word in the text. The sequences are then padded to ensure that all input sequences are of the same length, which is required for the LSTM model.\n\n```python\n# Tokenize and pad sequences\ntokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(train_data['Processed_Text'])\nX_train_seq = tokenizer.texts_to_sequences(train_data['Processed_Text'])\nX_dev_seq = tokenizer.texts_to_sequences(dev_data['Processed_Text'])\n\nX_train_seq = pad_sequences(X_train_seq, maxlen=100)\nX_dev_seq = pad_sequences(X_dev_seq, maxlen=100)\n```\n\n### LSTM Model Definition\nWe define a Sequential LSTM model with the following layers:\n1. **Embedding Layer**: Converts integer sequences to dense vectors of fixed size.\n2. **LSTM Layer**: Captures temporal dependencies in the sequence data.\n3. **GlobalMaxPool1D Layer**: Reduces the output from the LSTM layer to a fixed-size vector by taking the maximum value over the time dimension.\n4. **Dense Layer with ReLU Activation**: Adds non-linearity to the model.\n5. **Dense Layer with Sigmoid Activation**: Outputs probabilities for each label.\n\nThe model is compiled with the binary cross-entropy loss function and the Adam optimizer. The accuracy metric is used for evaluation during training.\n\n```python\n# LSTM Model Definition\ndef create_lstm_model(input_length, output_length):\n    model = Sequential([\n        Embedding(input_dim=5000, output_dim=128, input_length=input_length),\n        LSTM(128, return_sequences=True),\n        GlobalMaxPool1D(),\n        Dense(128, activation='relu'),\n        Dense(output_length, activation='sigmoid')\n    ])\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\nmax_seq_length = 100\nlstm_model = create_lstm_model(max_seq_length, y_train.shape[1])\n```\n\n### Model Training\nThe model is trained for 25 epochs with a batch size of 32. Training involves feeding the model with the training sequences and labels, and validating on the development sequences and labels.\n\n```python\nlstm_model.fit(X_train_seq, y_train, epochs=25, batch_size=32, validation_data=(X_dev_seq, y_dev))\n```\n\n### Evaluation\nThe trained model is evaluated on the development data using various metrics:\n- **Accuracy**: Proportion of correctly predicted labels.\n- **Precision**: Proportion of true positive predictions among all positive predictions.\n- **Recall**: Proportion of true positive predictions among all actual positives.\n- **F1 Score**: Harmonic mean of precision and recall.\n- **Hamming Loss**: Fraction of incorrect labels to the total number of labels.\n- **AUC-ROC**: Area under the receiver operating characteristic curve, averaged across labels.\n\n```python\n# Evaluate LSTM model\ndef evaluate_lstm_model(model, X_dev, y_dev):\n    predictions = (model.predict(X_dev) > 0.5).astype(\"int32\")\n    accuracy = accuracy_score(y_dev, predictions)\n    precision = precision_score(y_dev, predictions, average='weighted', zero_division=0)\n    recall = recall_score(y_dev, predictions, average='weighted', zero_division=0)\n    f1 = f1_score(y_dev, predictions, average='weighted', zero_division=0)\n    hamming = hamming_loss(y_dev, predictions)\n    auc_roc = roc_auc_score(y_dev, predictions, average='weighted', multi_class='ovr')\n    \n    print(\"LSTM Model Accuracy:\", accuracy)\n    print(\"LSTM Model Precision:\", precision)\n    print(\"LSTM Model Recall:\", recall)\n    print(\"LSTM Model F1 Score:\", f1)\n    print(\"LSTM Model Hamming Loss:\", hamming)\n    print(\"LSTM Model AUC-ROC:\", auc_roc)\n\nevaluate_lstm_model(lstm_model, X_dev_seq, y_dev)\n```\n\n### Explanation and Benefits\n- **Number of Epochs**: The model is trained for 25 epochs. An epoch refers to one complete pass through the entire training dataset. Training for multiple epochs helps the model learn and generalize better, though overfitting must be monitored.\n- **Layers**: The model comprises an Embedding layer, an LSTM layer, a GlobalMaxPool1D layer, and two Dense layers. The LSTM layer captures sequential dependencies, making it suitable for text data. The Embedding layer converts words to dense vectors, and the Dense layers add non-linearity and produce final predictions.\n- **Benefits**: \n  - **LSTM**: Suitable for sequential data, capturing long-range dependencies and relationships in text.\n  - **Embedding**: Reduces the dimensionality of text data while preserving semantic relationships between words.\n  - **GlobalMaxPool1D**: Reduces the sequence dimension by taking the maximum value, helping to focus on the most salient features.\n  - **Hyperparameter Choices**: Batch size and epochs are chosen based on typical training practices and constraints like memory and time.\n\nUsing an LSTM model helps in effectively modeling the temporal dependencies in text data, making it a robust choice for tasks like text classification, especially in multi-label settings.\n```\n","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, GlobalMaxPool1D\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, hamming_loss, roc_auc_score\n\n# Tokenize and pad sequences\ntokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(train_data['Processed_Text'])\nX_train_seq = tokenizer.texts_to_sequences(train_data['Processed_Text'])\nX_dev_seq = tokenizer.texts_to_sequences(dev_data['Processed_Text'])\n\nX_train_seq = pad_sequences(X_train_seq, maxlen=100)\nX_dev_seq = pad_sequences(X_dev_seq, maxlen=100)\n\n# LSTM Model Definition\ndef create_lstm_model(input_length, output_length):\n    model = Sequential([\n        Embedding(input_dim=5000, output_dim=128, input_length=input_length),\n        LSTM(128, return_sequences=True),\n        GlobalMaxPool1D(),\n        Dense(128, activation='relu'),\n        Dense(output_length, activation='sigmoid')\n    ])\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\nmax_seq_length = 100\nlstm_model = create_lstm_model(max_seq_length, y_train.shape[1])\nlstm_model.fit(X_train_seq, y_train, epochs=25, batch_size=32, validation_data=(X_dev_seq, y_dev))\n\n# Evaluate LSTM model\ndef evaluate_lstm_model(model, X_dev, y_dev):\n    predictions = (model.predict(X_dev) > 0.5).astype(\"int32\")\n    accuracy = accuracy_score(y_dev, predictions)\n    precision = precision_score(y_dev, predictions, average='weighted', zero_division=0)\n    recall = recall_score(y_dev, predictions, average='weighted', zero_division=0)\n    f1 = f1_score(y_dev, predictions, average='weighted', zero_division=0)\n    hamming = hamming_loss(y_dev, predictions)\n    auc_roc = roc_auc_score(y_dev, predictions, average='weighted', multi_class='ovr')\n    \n    print(\"LSTM Model Accuracy:\", accuracy)\n    print(\"LSTM Model Precision:\", precision)\n    print(\"LSTM Model Recall:\", recall)\n    print(\"LSTM Model F1 Score:\", f1)\n    print(\"LSTM Model Hamming Loss:\", hamming)\n    print(\"LSTM Model AUC-ROC:\", auc_roc)\n\nevaluate_lstm_model(lstm_model, X_dev_seq, y_dev)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-14T13:17:39.803633Z","iopub.execute_input":"2024-07-14T13:17:39.803922Z","iopub.status.idle":"2024-07-14T13:22:16.910775Z","shell.execute_reply.started":"2024-07-14T13:17:39.803898Z","shell.execute_reply":"2024-07-14T13:22:16.909819Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Epoch 1/25\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 8ms/step - accuracy: 0.2785 - loss: 0.1756 - val_accuracy: 0.4125 - val_loss: 0.1302\nEpoch 2/25\n\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.4381 - loss: 0.1249 - val_accuracy: 0.4814 - val_loss: 0.1151\nEpoch 3/25\n\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.4881 - loss: 0.1107 - val_accuracy: 0.5118 - val_loss: 0.1084\nEpoch 4/25\n\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.5178 - loss: 0.1027 - val_accuracy: 0.5160 - val_loss: 0.1062\nEpoch 5/25\n\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.5353 - loss: 0.0978 - val_accuracy: 0.5210 - val_loss: 0.1052\nEpoch 6/25\n\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.5475 - loss: 0.0935 - val_accuracy: 0.5249 - val_loss: 0.1057\nEpoch 7/25\n\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.5664 - loss: 0.0886 - val_accuracy: 0.5123 - val_loss: 0.1054\nEpoch 8/25\n\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.5842 - loss: 0.0853 - val_accuracy: 0.5116 - val_loss: 0.1068\nEpoch 9/25\n\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.5963 - loss: 0.0818 - val_accuracy: 0.5033 - val_loss: 0.1086\nEpoch 10/25\n\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.6135 - loss: 0.0783 - val_accuracy: 0.5050 - val_loss: 0.1106\nEpoch 11/25\n\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.6283 - loss: 0.0755 - val_accuracy: 0.5002 - val_loss: 0.1132\nEpoch 12/25\n\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.6429 - loss: 0.0718 - val_accuracy: 0.4821 - val_loss: 0.1160\nEpoch 13/25\n\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.6587 - loss: 0.0687 - val_accuracy: 0.4891 - val_loss: 0.1197\nEpoch 14/25\n\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.6756 - loss: 0.0656 - val_accuracy: 0.4794 - val_loss: 0.1232\nEpoch 15/25\n\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.6944 - loss: 0.0621 - val_accuracy: 0.4792 - val_loss: 0.1289\nEpoch 16/25\n\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.7047 - loss: 0.0597 - val_accuracy: 0.4757 - val_loss: 0.1334\nEpoch 17/25\n\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.7226 - loss: 0.0558 - val_accuracy: 0.4593 - val_loss: 0.1390\nEpoch 18/25\n\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.7411 - loss: 0.0526 - val_accuracy: 0.4624 - val_loss: 0.1448\nEpoch 19/25\n\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.7512 - loss: 0.0503 - val_accuracy: 0.4383 - val_loss: 0.1523\nEpoch 20/25\n\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.7681 - loss: 0.0466 - val_accuracy: 0.4453 - val_loss: 0.1586\nEpoch 21/25\n\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.7757 - loss: 0.0445 - val_accuracy: 0.4484 - val_loss: 0.1644\nEpoch 22/25\n\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.7886 - loss: 0.0415 - val_accuracy: 0.4414 - val_loss: 0.1756\nEpoch 23/25\n\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.7960 - loss: 0.0395 - val_accuracy: 0.4370 - val_loss: 0.1811\nEpoch 24/25\n\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.8057 - loss: 0.0369 - val_accuracy: 0.4289 - val_loss: 0.1870\nEpoch 25/25\n\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.8154 - loss: 0.0346 - val_accuracy: 0.4292 - val_loss: 0.1984\n\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\nLSTM Model Accuracy: 0.32012532252119424\nLSTM Model Precision: 0.4666204752317041\nLSTM Model Recall: 0.4068965517241379\nLSTM Model F1 Score: 0.4295911735235144\nLSTM Model Hamming Loss: 0.04259912590174293\nLSTM Model AUC-ROC: 0.662158030834902\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install tensorflow_text","metadata":{"execution":{"iopub.status.busy":"2024-07-14T13:23:02.323746Z","iopub.execute_input":"2024-07-14T13:23:02.324110Z","iopub.status.idle":"2024-07-14T13:23:14.632565Z","shell.execute_reply.started":"2024-07-14T13:23:02.324081Z","shell.execute_reply":"2024-07-14T13:23:14.631562Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: tensorflow_text in /opt/conda/lib/python3.10/site-packages (2.15.0)\nRequirement already satisfied: tensorflow-hub>=0.13.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow_text) (0.16.1)\nRequirement already satisfied: tensorflow<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow_text) (2.15.0)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (23.5.26)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (0.5.4)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (0.2.0)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (3.10.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (16.0.6)\nRequirement already satisfied: ml-dtypes~=0.2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (0.2.0)\nRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (1.26.4)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (69.0.3)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (1.16.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (2.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (4.9.0)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (1.14.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (0.35.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (1.60.0)\nRequirement already satisfied: tensorboard<2.16,>=2.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (2.15.1)\nRequirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (2.15.0)\nRequirement already satisfied: keras<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text) (2.15.0)\nRequirement already satisfied: tf-keras>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow-hub>=0.13.0->tensorflow_text) (2.15.1)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow<2.16,>=2.15.0->tensorflow_text) (0.42.0)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (2.26.1)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (3.5.2)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (2.32.3)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (3.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow<2.16,>=2.15.0->tensorflow_text) (3.1.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (2024.7.4)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (2.1.3)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text) (3.2.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install wurlitzer","metadata":{"execution":{"iopub.status.busy":"2024-07-14T13:23:14.634617Z","iopub.execute_input":"2024-07-14T13:23:14.634945Z","iopub.status.idle":"2024-07-14T13:23:26.875282Z","shell.execute_reply.started":"2024-07-14T13:23:14.634917Z","shell.execute_reply":"2024-07-14T13:23:26.874276Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Requirement already satisfied: wurlitzer in /opt/conda/lib/python3.10/site-packages (3.1.1)\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install optuna\n","metadata":{"execution":{"iopub.status.busy":"2024-07-14T13:23:26.876703Z","iopub.execute_input":"2024-07-14T13:23:26.877019Z","iopub.status.idle":"2024-07-14T13:23:38.955484Z","shell.execute_reply.started":"2024-07-14T13:23:26.876991Z","shell.execute_reply":"2024-07-14T13:23:38.954326Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Requirement already satisfied: optuna in /opt/conda/lib/python3.10/site-packages (3.6.1)\nRequirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (1.13.2)\nRequirement already satisfied: colorlog in /opt/conda/lib/python3.10/site-packages (from optuna) (6.8.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from optuna) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (21.3)\nRequirement already satisfied: sqlalchemy>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (2.0.25)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from optuna) (4.66.4)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from optuna) (6.0.1)\nRequirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (1.3.5)\nRequirement already satisfied: typing-extensions>=4 in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->optuna) (3.1.1)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.10/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Multi-Label Text Classification using BERT and Hyperparameter Tuning with Optuna\n\n## Overview\nThis notebook provides a comprehensive guide for performing multi-label text classification using BERT. The workflow includes loading and preprocessing data, tokenizing text with the BERT tokenizer, creating data loaders, defining the BERT model, and optimizing hyperparameters using Optuna.\n\n## Setup\nWe begin by importing the necessary libraries and setting up the device configuration (CUDA if available).\n\n## Loading Data\nWe load the training and development datasets from TSV files. The dataset does not contain any column name, therefore we assigned the column name based on the columns : `Text`, `Num`, and `ID`.\n\n## Data Preprocessing\nWe process the `Num` column to convert it from a string of comma-separated numbers to a list of integers. This transformation is essential for multi-label binarization.\n\n## Label Binarization\nUsing `MultiLabelBinarizer`, we convert the lists of integers into a binary format suitable for multi-label classification. Here, we assume there are 28 possible labels (we got this from matching the num columns with the emotions in emotion.txt).\n\n## Tokenization\nWe load the BERT tokenizer (`bert-base-uncased`) and define a function to tokenize and encode the texts. This function adds special tokens, pads/truncates the texts to a maximum length of 128, and returns attention masks.\n\n## Creating DataLoaders\nWe create `TensorDataset` and `DataLoader` for both training and development data. We use a batch size of 8 to fit the data into memory. DataLoaders are crucial for efficient batching and shuffling during training and evaluation.\n\n## Model Definition\nWe define a BERT model for sequence classification with the number of labels equal to the number of emotions (28). The model is wrapped with `torch.nn.DataParallel` for parallel processing if multiple GPUs are available.\n\n## Training Configuration\n- **Epochs**: We train the model for 3 epochs.\n- **Layers**: The BERT model consists of 12 transformer layers.\n- **Optimization**: We use the AdamW optimizer and a linear scheduler with warm-up steps.\n\n## Hyperparameter Tuning with Optuna\nOptuna is used for hyperparameter optimization due to its efficient and flexible search capabilities. Optuna allows us to define an objective function, which is optimized over multiple trials to find the best hyperparameters.\n\n### Why Optuna?\nOptuna provides:\n- Automatic handling of hyperparameter optimization.\n- Efficient search algorithms (e.g., Tree-structured Parzen Estimator).\n- Easy integration with existing codebases.\n- Ability to handle various types of hyperparameters (e.g., categorical, continuous).\n\n### Hyperparameters Tuned\n- **Learning Rate**: Suggested in the range [1e-5, 1e-4].\n- **Batch Size**: Suggested values [4, 8, 16].\n\n### Training Steps Calculation\nThe total number of training steps is calculated as:\n```python\ntotal_steps = len(train_dataloader) * 3\n```\nThis means the total number of steps is the length of the training DataLoader multiplied by the number of epochs (3 in this case). This helps in setting up the scheduler for learning rate adjustments throughout the training process.\n- **n_trials:** The number of trials Optuna will run. Each trial represents one complete run of the training process with a specific set of hyperparameters.\n\n## Evaluation Metrics\nThe model's performance is evaluated using:\n\n- **Accuracy**\n- **Precision (Weighted)**\n- **Recall (Weighted)**\n- **F1 Score (Weighted)**\n- **Hamming Loss**\n- **AUC-ROC (Weighted)**\n- These metrics provide a comprehensive understanding of the model's performance on multi-label classification tasks.\n\n## Conclusion\nThis notebook demonstrates the complete pipeline for multi-label text classification using BERT, enhanced with hyperparameter tuning via Optuna. It includes data loading, preprocessing, tokenization, model training, and evaluation. Hyperparameter tuning with Optuna helps in finding the optimal learning rate and batch size, thereby improving the model's performance.\n","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\nimport pandas as pd\nfrom sklearn.preprocessing import MultiLabelBinarizer\nimport numpy as np\nimport optuna\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, hamming_loss, roc_auc_score\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load data\nprint(\"Loading data...\")\ntrain_data = pd.read_csv('/kaggle/input/assignment/data/train.tsv', sep='\\t', header=None, names=['Text', 'Num', 'ID'])\ndev_data = pd.read_csv('/kaggle/input/assignment/data/dev.tsv', sep='\\t', header=None, names=['Text', 'Num', 'ID'])\nprint(\"Data loaded.\")\n\n# Convert Num column to list of integers\nprint(\"Processing Num column...\")\ntrain_data['Num'] = train_data['Num'].apply(lambda x: list(map(int, x.split(','))))\ndev_data['Num'] = dev_data['Num'].apply(lambda x: list(map(int, x.split(','))))\nprint(\"Num column processed.\")\n\n# Multi-Label Binarizer for the labels\nprint(\"Applying MultiLabelBinarizer...\")\nemotions = [i for i in range(28)]\nmlb = MultiLabelBinarizer(classes=emotions)\ny_train = mlb.fit_transform(train_data['Num'])\ny_dev = mlb.transform(dev_data['Num'])\nprint(\"MultiLabelBinarizer applied.\")\n\n# Tokenizer\nprint(\"Loading BERT tokenizer...\")\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nprint(\"Tokenizer loaded.\")\n\ndef encode_texts(texts, tokenizer, max_length=128):\n    input_ids, attention_masks = [], []\n    for text in texts:\n        encoded = tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=max_length,\n            padding='max_length',\n            return_attention_mask=True,\n            return_tensors='pt',\n            truncation=True\n        )\n        input_ids.append(encoded['input_ids'])\n        attention_masks.append(encoded['attention_mask'])\n    return torch.cat(input_ids, dim=0), torch.cat(attention_masks, dim=0)\n\n# Encode data\nprint(\"Encoding training data...\")\nX_train_bert, X_train_attention = encode_texts(train_data['Text'], tokenizer)\nprint(\"Training data encoded.\")\nprint(\"Encoding development data...\")\nX_dev_bert, X_dev_attention = encode_texts(dev_data['Text'], tokenizer)\nprint(\"Development data encoded.\")\n\n# Create DataLoader\nprint(\"Creating DataLoaders...\")\ntrain_dataset = TensorDataset(X_train_bert, X_train_attention, torch.tensor(y_train, dtype=torch.float32))\ndev_dataset = TensorDataset(X_dev_bert, X_dev_attention, torch.tensor(y_dev, dtype=torch.float32))\nbatch_size = 8  # Reduce batch size to fit into memory\ntrain_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)\ndev_dataloader = DataLoader(dev_dataset, sampler=SequentialSampler(dev_dataset), batch_size=batch_size)\nprint(\"DataLoaders created.\")\n\n# Objective function for hyperparameter tuning\ndef objective(trial):\n    # Hyperparameters to tune\n    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-4)\n    batch_size = trial.suggest_categorical('batch_size', [4, 8, 16])\n\n    # Model\n    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(emotions))\n    model = torch.nn.DataParallel(model)\n    model.to(device)\n\n    # Optimizer and scheduler\n    optimizer = AdamW(model.parameters(), lr=learning_rate, eps=1e-8)\n    total_steps = len(train_dataloader) * 3\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n\n    print(f\"Trial {trial.number}: Learning rate: {learning_rate}, Batch size: {batch_size}\")\n\n    for epoch in range(3):\n        model.train()\n        total_loss = 0\n        for step, batch in enumerate(train_dataloader):\n            if step % 1000 == 0 and not step == 0:\n                print(f\"  Batch {step}  of  {len(train_dataloader)}.\")\n            b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n            model.zero_grad()\n            outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n            loss = outputs.loss.mean()  # Average the loss if it's a multi-element tensor\n            total_loss += loss.item()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            scheduler.step()\n            torch.cuda.empty_cache()\n\n    avg_train_loss = total_loss / len(train_dataloader)\n    print(f\"Epoch {epoch+1} complete. Average training loss: {avg_train_loss}\")\n\n    model.eval()\n    predictions, true_labels = [], []\n    for batch in dev_dataloader:\n        b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n        with torch.no_grad():\n            outputs = model(b_input_ids, attention_mask=b_input_mask)\n        logits = outputs.logits\n        predictions.append(logits.cpu().numpy())\n        true_labels.append(b_labels.cpu().numpy())\n\n    predictions = np.concatenate(predictions, axis=0)\n    true_labels = np.concatenate(true_labels, axis=0)\n\n    accuracy = accuracy_score(true_labels, (predictions > 0.5).astype(int))\n    precision = precision_score(true_labels, (predictions > 0.5).astype(int), average='weighted', zero_division=0)\n    recall = recall_score(true_labels, (predictions > 0.5).astype(int), average='weighted', zero_division=0)\n    f1 = f1_score(true_labels, (predictions > 0.5).astype(int), average='weighted', zero_division=0)\n    hamming = hamming_loss(true_labels, (predictions > 0.5).astype(int))\n    auc_roc = roc_auc_score(true_labels, predictions, average='weighted', multi_class='ovr')\n\n    print(\"Accuracy:\", accuracy)\n    print(\"Precision:\", precision)\n    print(\"Recall:\", recall)\n    print(\"F1 Score:\", f1)\n    print(\"Hamming Loss:\", hamming)\n    print(\"AUC-ROC:\", auc_roc)\n\n    return f1\n\n# Hyperparameter tuning with Optuna\nprint(\"Starting hyperparameter tuning...\")\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=3)\nprint(\"Hyperparameter tuning complete.\")\nprint(\"Best trial:\", study.best_trial.params)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-14T16:48:48.723168Z","iopub.execute_input":"2024-07-14T16:48:48.724145Z","iopub.status.idle":"2024-07-14T20:05:55.674278Z","shell.execute_reply.started":"2024-07-14T16:48:48.724101Z","shell.execute_reply":"2024-07-14T20:05:55.673420Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Loading data...\nData loaded.\nProcessing Num column...\nNum column processed.\nApplying MultiLabelBinarizer...\nMultiLabelBinarizer applied.\nLoading BERT tokenizer...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2740cd79cb204f90b8db241c041f9bd0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05982c94743d4bb88d6424e07a6192cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b7b7cc4dc5941959c357d5cb2edf9bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d116599806764447bf914c588ffdcb6a"}},"metadata":{}},{"name":"stdout","text":"Tokenizer loaded.\nEncoding training data...\nTraining data encoded.\nEncoding development data...\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-07-14 16:49:30,908] A new study created in memory with name: no-name-beb0f2cd-0862-4fc9-9346-5a344a6b14de\n","output_type":"stream"},{"name":"stdout","text":"Development data encoded.\nCreating DataLoaders...\nDataLoaders created.\nStarting hyperparameter tuning...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_34/203356172.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-4)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"551bba65d6214f3fb324f28bc2ecaa26"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Trial 0: Learning rate: 2.4085688833612367e-05, Batch size: 8\n","output_type":"stream"},{"name":"stderr","text":"2024-07-14 16:49:38.258732: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-14 16:49:38.258879: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-14 16:49:38.439514: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"  Batch 1000  of  5427.\n  Batch 2000  of  5427.\n  Batch 3000  of  5427.\n  Batch 4000  of  5427.\n  Batch 5000  of  5427.\n  Batch 1000  of  5427.\n  Batch 2000  of  5427.\n  Batch 3000  of  5427.\n  Batch 4000  of  5427.\n  Batch 5000  of  5427.\n  Batch 1000  of  5427.\n  Batch 2000  of  5427.\n  Batch 3000  of  5427.\n  Batch 4000  of  5427.\n  Batch 5000  of  5427.\nEpoch 3 complete. Average training loss: 0.06411474052252379\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-07-14 17:55:02,630] Trial 0 finished with value: 0.5237860871724288 and parameters: {'learning_rate': 2.4085688833612367e-05, 'batch_size': 8}. Best is trial 0 with value: 0.5237860871724288.\n","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.42978252856616295\nPrecision: 0.6999988787328989\nRecall: 0.44655172413793104\nF1 Score: 0.5237860871724288\nHamming Loss: 0.030027381391185298\nAUC-ROC: 0.907734549258119\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_34/203356172.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-4)\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"Trial 1: Learning rate: 1.5905951682749565e-05, Batch size: 4\n  Batch 1000  of  5427.\n  Batch 2000  of  5427.\n  Batch 3000  of  5427.\n  Batch 4000  of  5427.\n  Batch 5000  of  5427.\n  Batch 1000  of  5427.\n  Batch 2000  of  5427.\n  Batch 3000  of  5427.\n  Batch 4000  of  5427.\n  Batch 5000  of  5427.\n  Batch 1000  of  5427.\n  Batch 2000  of  5427.\n  Batch 3000  of  5427.\n  Batch 4000  of  5427.\n  Batch 5000  of  5427.\nEpoch 3 complete. Average training loss: 0.07004395058446877\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-07-14 19:00:28,916] Trial 1 finished with value: 0.5034540426598872 and parameters: {'learning_rate': 1.5905951682749565e-05, 'batch_size': 4}. Best is trial 0 with value: 0.5237860871724288.\n","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.413932915591596\nPrecision: 0.7321844228299753\nRecall: 0.42664576802507836\nF1 Score: 0.5034540426598872\nHamming Loss: 0.02992206834816492\nAUC-ROC: 0.9051792891506547\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_34/203356172.py:74: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-4)\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"Trial 2: Learning rate: 3.24639062094172e-05, Batch size: 16\n  Batch 1000  of  5427.\n  Batch 2000  of  5427.\n  Batch 3000  of  5427.\n  Batch 4000  of  5427.\n  Batch 5000  of  5427.\n  Batch 1000  of  5427.\n  Batch 2000  of  5427.\n  Batch 3000  of  5427.\n  Batch 4000  of  5427.\n  Batch 5000  of  5427.\n  Batch 1000  of  5427.\n  Batch 2000  of  5427.\n  Batch 3000  of  5427.\n  Batch 4000  of  5427.\n  Batch 5000  of  5427.\nEpoch 3 complete. Average training loss: 0.05993329501972193\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-07-14 20:05:55,670] Trial 2 finished with value: 0.5296023362779754 and parameters: {'learning_rate': 3.24639062094172e-05, 'batch_size': 16}. Best is trial 2 with value: 0.5296023362779754.\n","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.43826022852930335\nPrecision: 0.681970764345205\nRecall: 0.45658307210031346\nF1 Score: 0.5296023362779754\nHamming Loss: 0.030382812911379075\nAUC-ROC: 0.9075687764340806\nHyperparameter tuning complete.\nBest trial: {'learning_rate': 3.24639062094172e-05, 'batch_size': 16}\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip freeze > /kaggle/working/requirements.txt \n","metadata":{"execution":{"iopub.status.busy":"2024-07-15T04:45:04.550012Z","iopub.execute_input":"2024-07-15T04:45:04.550388Z","iopub.status.idle":"2024-07-15T04:45:07.875615Z","shell.execute_reply.started":"2024-07-15T04:45:04.550352Z","shell.execute_reply":"2024-07-15T04:45:07.874423Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}